{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Rafael PÃ©rez and Pablo Sarricolea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import libtlda\n",
    "from libtlda.tca import TransferComponentClassifier\n",
    "import scipy.signal as sig\n",
    "import scipy.fftpack as fp\n",
    "import os\n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from random import randrange\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants needed depending on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fs=160\n",
    "samples = 20000 #30 secs\n",
    "directory_name='S001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEEG(file_name, file_labels,fs, samples):\n",
    "    \n",
    "    file = pyedflib.EdfReader(file_name)\n",
    "    nOfSignals = file.signals_in_file\n",
    "    sigbufs = np.zeros((nOfSignals,samples))\n",
    "    for i in np.arange(nOfSignals):\n",
    "        sigbufs[i,0:file.getNSamples()[0]]=file.readSignal(i)\n",
    "    #sigbufs=sigbufs[:,0:samples]   \n",
    "    x = np.arange(len(sigbufs[0]))/fs*1000\n",
    "    labels = np.loadtxt(file_labels,dtype=str)\n",
    "    sup = np.array(labels[:,1],dtype = float)\n",
    "    sup=sup/160*1000\n",
    "    labels[:,1]=sup\n",
    "    labels=labels[:,(1,2,-1)]\n",
    "\n",
    "\n",
    "    for i in range(len(labels[:,1])):\n",
    "        if (labels[i,1]==\"T0\"):\n",
    "            labels[i,1]=0\n",
    "        elif (labels[i,1]==\"T1\"):\n",
    "            labels[i,1]=1\n",
    "        else: labels[i,1]=2\n",
    "    \n",
    "    \n",
    "    labels = np.matrix(labels,dtype=float)\n",
    "    labels[:,2]*=1000\n",
    "    labels=np.matrix(labels,dtype=int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return x,sigbufs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDirectory(directory_name,fs,samples):\n",
    "    fileEDF=glob.glob('S001/*.edf')\n",
    "    \n",
    "    for i in range(len(fileEDF)):\n",
    "        fileEDF[i]=\"rdedfann -r \"+ fileEDF[i]+\" -F 160 >> \"+ fileEDF[i]+\".txt\"\n",
    "    \n",
    "    file = open(\"S001/rdedfann.sh\",\"w\")\n",
    "\n",
    "    file.write(\"#!/bin/bash\\n\")\n",
    "\n",
    "    for i in fileEDF:\n",
    "        file.write(i+'\\n')\n",
    "\n",
    "    file.close() \n",
    "\n",
    "    #move to the right directory\n",
    "    #in bash: fromdos rdedfann.sh\n",
    "    #sh rdedfann.sh\n",
    "    fileEDF=glob.glob('S001/*.edf')\n",
    "    fileLabels=glob.glob('S001/*.txt')\n",
    "    patients = len(fileEDF)\n",
    "    channels=64*patients\n",
    "    eegMatrix =  np.zeros(shape=(64*patients,samples))\n",
    "    j=0\n",
    "    labelsF=np.zeros(shape=(patients,samples))\n",
    "\n",
    "    for i in range(patients):\n",
    "        \n",
    "        _,eegMatrix[j:(i+1)*64,:],labels=loadEEG(fileEDF[i],fileLabels[i],fs,samples)\n",
    "        index =0\n",
    "\n",
    "        for k in range(len(labels)):\n",
    "            nSamples=int(labels[k,2]/(1000/fs))\n",
    "            labelsF[i,index:(index+nSamples-1)]=labels[k,1]*1\n",
    "            index+=nSamples\n",
    "        \n",
    "        j=(i+1)*64\n",
    "    return eegMatrix,labelsF\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "eegMatrix,labels=loadDirectory('S001',fs,samples)\n",
    "\n",
    "print(eegMatrix.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNotch=60\n",
    "fcs = fNotch/(fs/2)\n",
    "nfilterb,nfiltera = sig.iirnotch(fcs,30)\n",
    "fcs = 0.05 / (fs / 2.) \n",
    "b, a = sig.butter(5, fcs, 'highpass')\n",
    "\n",
    "for i in range(896):\n",
    "    eegMatrix[i,:] = sig.lfilter(nfilterb,nfiltera,eegMatrix[i,:])\n",
    "    eegMatrix[i,:] = sig.filtfilt (b, a, eegMatrix[i,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegMatrixS=eegMatrix\n",
    "scaler=sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "print(eegMatrixS.shape)\n",
    "scaler.fit(eegMatrixS.T)\n",
    "eegMatrixS=scaler.transform(eegMatrixS.T)\n",
    "print(eegMatrixS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels.T\n",
    "chunks=15\n",
    "int(np.floor(samples/chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCA approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutInChunks(Xs,Xv,Xt,Ys,Yv,Yt) :   \n",
    "    idx=0\n",
    "    idx2=0\n",
    "    chunkXsn=[]\n",
    "    chunkXtn=[]\n",
    "    chunkXvn=[]\n",
    "    chunkYs=[]\n",
    "    chunkYt=[]\n",
    "    chunkYv=[]\n",
    "    print(patTrain)\n",
    "    print(Ys.shape)\n",
    "    for j in range(patTrain):\n",
    "        for i in (int(np.floor(samples/chunks)))*np.arange(1,chunks+1):\n",
    "            chunkXsn.append(Xs[int(idx):int(i),idx2:idx2+64])\n",
    "            chunkXtn.append(Xt[int(idx):int(i),idx2:idx2+64])\n",
    "            chunkXvn.append(Xv[int(idx):int(i),idx2:idx2+64])\n",
    "            if (np.max(Ys[int(idx):int(i),j])==1): \n",
    "                chunkYs.append(1)\n",
    "            elif (np.max(Ys[int(idx):int(i),j])==2):\n",
    "                chunkYs.append(2)\n",
    "            idx=i\n",
    "        idx=0\n",
    "        idx2+=64\n",
    "\n",
    "\n",
    "\n",
    "    idx=0\n",
    "    for i in (int(np.floor(samples/chunks)))*np.arange(1,chunks+1):\n",
    "        if (np.max(Yt[int(idx):int(i)])==1): \n",
    "            chunkYt.append(1)\n",
    "        elif (np.max(Yt[int(idx):int(i)])==2):\n",
    "            chunkYt.append(2)\n",
    "        idx=i\n",
    "        \n",
    "        \n",
    "    idx=0\n",
    "    for i in (int(np.floor(samples/chunks)))*np.arange(1,chunks+1):\n",
    "        if (np.max(Yv[int(idx):int(i)])==1): \n",
    "            chunkYv.append(1)\n",
    "        elif (np.max(Yv[int(idx):int(i)])==2):\n",
    "            chunkYv.append(2)\n",
    "        idx=i\n",
    "\n",
    "    chunkXsn=np.array(chunkXsn)\n",
    "    chunkXtn=np.array(chunkXtn)\n",
    "    chunkXvn=np.array(chunkXvn)\n",
    "    chunkYs=np.array(chunkYs).reshape((len(chunkYs),1))\n",
    "    chunkYt=np.array(chunkYt).reshape((len(chunkYt),1))\n",
    "    chunkYv=np.array(chunkYt).reshape((len(chunkYv),1))\n",
    "    \n",
    "    print(chunkXsn.shape)\n",
    "    print(chunkXtn.shape)\n",
    "    print(chunkYs.shape)\n",
    "    print(chunkXvn.shape)\n",
    "    print(chunkYv.shape)\n",
    "    \n",
    "    return chunkXsn,chunkXvn,chunkXtn,chunkYs,chunkYv,chunkYt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rightLeft(chunkXsn,chunkXtn,chunkYs,chunkYt):\n",
    "    #ZERO PADDING\n",
    "    idl,_=np.array(np.where(chunkYs==1))\n",
    "    idr,_=np.array(np.where(chunkYs==2))\n",
    "\n",
    "    chunkLefts=chunkXsn[(idl),:,:]\n",
    "    chunkRights=chunkXsn[idr,:,:]\n",
    "\n",
    "    print(chunkLefts.shape)\n",
    "    print(chunkRights.shape)\n",
    "\n",
    "    idl2,_=np.array(np.where(chunkYt==1))\n",
    "    idr2,_=np.array(np.where(chunkYt==2))\n",
    "\n",
    "    chunkLeftt = np.zeros(shape=(chunkLefts.shape))\n",
    "    chunkRightt = np.zeros(shape=(chunkRights.shape))\n",
    "\n",
    "    print(len(idr))\n",
    "\n",
    "    chunkLeftt[0:len(idl2),:,:]=chunkXtn[(idl2),:,:]\n",
    "    chunkRightt[0:len(idr2),:,:]=chunkXtn[(idr2),:,:]\n",
    "\n",
    "    print(chunkLeftt.shape)\n",
    "    print(chunkRightt.shape)\n",
    "\n",
    "    \n",
    "    #chunkRights=chunkRights.T.reshape((int(np.floor(samples/chunks)),len(idr)*channels))\n",
    "    #chunkLefts=chunkLefts.T.reshape((int(np.floor(samples/chunks)),len(idl)*channels))\n",
    "    \n",
    "    chunkAuxR = chunkRights[0,:,:]\n",
    "    chunkAuxL = chunkLefts[0,:,:]\n",
    "    \n",
    "    for i in np.arange(1,len(idr)):\n",
    "        chunkAuxR = np.concatenate((chunkAuxR,chunkRights[i,:,:]),axis=1)\n",
    "    \n",
    "    for i in np.arange(1,len(idl)):\n",
    "        chunkAuxL = np.concatenate((chunkAuxL,chunkLefts[i,:,:]),axis=1)\n",
    "        \n",
    "    chunkRights = chunkAuxR\n",
    "    chunkLefts = chunkAuxL\n",
    "    \n",
    "    chunkAuxR = chunkRightt[0,:,:]\n",
    "    chunkAuxL = chunkLeftt[0,:,:]\n",
    "    \n",
    "    for i in np.arange(1,len(idr2)):\n",
    "        chunkAuxR = np.concatenate((chunkAuxR,chunkRightt[i,:,:]),axis=1)\n",
    "    \n",
    "    for i in np.arange(1,len(idl2)):\n",
    "        chunkAuxL = np.concatenate((chunkAuxL,chunkRightt[i,:,:]),axis=1)\n",
    "    \n",
    "    chunkRightt = chunkAuxR\n",
    "    chunkLeftt = chunkAuxL\n",
    "    \n",
    "    #chunkRightt=chunkRightt.T.reshape((int(np.floor(samples/chunks)),len(idr)*channels))\n",
    "    #chunkLeftt=chunkLeftt.T.reshape((int(np.floor(samples/chunks)),len(idl)*channels))\n",
    "\n",
    "    print(chunkLefts.shape)\n",
    "    print(chunkLeftt.shape)\n",
    "    \n",
    "    return chunkRights,chunkLefts,chunkRightt,chunkLeftt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(chunkRightt,chunkLeftt,chunkRights,chunkLefts,pad=1):\n",
    "    if (pad==0):\n",
    "        chunkAux = np.ones(shape=chunkRights.shape)*np.mean(chunkRightt)\n",
    "        chunkAux[:,0:len(chunkRightt[0])]=chunkRightt\n",
    "        chunkRightt=chunkAux\n",
    "        chunkAux = np.ones(shape=chunkLefts.shape)*np.mean(chunkLeftt)\n",
    "        chunkAux[:,0:len(chunkLeftt[0])]=chunkLeftt\n",
    "        plt.plot(chunkAux[:,0])\n",
    "        plt.plot(chunkLeftt[:,0])\n",
    "        chunkLeftt=chunkAux\n",
    "    elif (pad == 1):\n",
    "        chunkAux = np.zeros(shape=chunkRights.shape)\n",
    "        chunkAux[:,0:len(chunkRightt[0])]=chunkRightt\n",
    "        chunkRightt=chunkAux\n",
    "        chunkAux = np.zeros(shape=chunkLefts.shape)\n",
    "        chunkAux[:,0:len(chunkLeftt[0])]=chunkLeftt\n",
    "        plt.plot(chunkAux[:,0])\n",
    "        plt.plot(chunkLeftt[:,0])\n",
    "        chunkLeftt=chunkAux\n",
    "    return chunkRightt, chunkLeftt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCselection(Xs,sig=0.81):\n",
    "    comp=0\n",
    "    data_rescaled=Xs.T\n",
    "    data_mean = np.mean(data_rescaled)\n",
    "    print(\"mean\")\n",
    "    print(data_mean)\n",
    "    data_center = data_rescaled - data_mean\n",
    "    cov_matrix = np.cov(data_center)\n",
    "    eigenval, eigenvec = np.linalg.eig(cov_matrix)\n",
    "    significance = [np.abs(i)/np.sum(eigenval) for i in eigenval]\n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(np.cumsum(significance))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.title('Pulsar Dataset Explained Variance')\n",
    "\n",
    "    count=0\n",
    "    for i in range(len(significance)):\n",
    "        count += significance[i]\n",
    "        if count > sig:\n",
    "            print(significance[i])\n",
    "            comp=i\n",
    "            break\n",
    "    plt.axvline(i,color='r')\n",
    "\n",
    "    plt.show()\n",
    "    return comp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCA (chunkS,chunkT,num_components,mu=1):\n",
    "    clf = TransferComponentClassifier(loss='logistic', mu=1,num_components=num_components)\n",
    "    C,K=clf.transfer_component_analysis(chunkS,chunkT)\n",
    "    #remaping\n",
    "    Xsn = np.dot(K[:int(np.floor(samples/chunks)), :], C)\n",
    "    XsXt=np.concatenate((chunkS,chunkT),axis=0)\n",
    "    K2 = clf.kernel(chunkT, XsXt  , type=clf.kernel_type,bandwidth=clf.bandwidth, order=clf.order)\n",
    "    Xtn = np.dot(K2, C)\n",
    "    \n",
    "    return Xsn,Xtn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleAfter(Xsnr,Xsnl,Xtnr,Xtnl):\n",
    "    scaler1=sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    scaler1.fit(Xsnr)\n",
    "    XsnrSc=scaler1.transform(Xsnr)\n",
    "    Xsnr = XsnrSc\n",
    "\n",
    "    scaler1=sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    scaler1.fit(Xsnl)\n",
    "    XsnlSc=scaler1.transform(Xsnl)\n",
    "    Xsnl = XsnlSc\n",
    "\n",
    "    scaler1=sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    scaler1.fit(Xtnr)\n",
    "    XtnrSc=scaler1.transform(Xtnr)\n",
    "    Xtnr = XtnrSc\n",
    "\n",
    "    scaler1=sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    scaler1.fit(Xtnl)\n",
    "    XtnlSc=scaler1.transform(Xtnl)\n",
    "    Xtnl = XtnlSc\n",
    "    \n",
    "    return Xsnr,Xsnl,Xtnr,Xtnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(Xsnr,Xsnl,Xtnr,Xtnl,Xvnr,Xvnl,num_compr,num_compl):\n",
    "    XsnrY=np.concatenate((Xsnr,np.ones(shape=(num_compr,1)).T))\n",
    "    XsnlY=np.concatenate((Xsnl,np.zeros(shape=(num_compl,1)).T))\n",
    "    XsrlY=np.concatenate((XsnlY,XsnrY),axis=1)\n",
    "    print(XsrlY.shape)\n",
    "\n",
    "    XtnrY=np.concatenate((Xtnr,np.ones(shape=(num_compr,1)).T))\n",
    "    XtnlY=np.concatenate((Xtnl,np.zeros(shape=(num_compl,1)).T))\n",
    "    XtrlY=np.concatenate((XtnlY,XtnrY),axis=1)\n",
    "    print(XsrlY.shape)\n",
    "    \n",
    "    XvnrY=np.concatenate((Xvnr,np.ones(shape=(num_compr,1)).T))\n",
    "    XvnlY=np.concatenate((Xvnl,np.zeros(shape=(num_compl,1)).T))\n",
    "    XvrlY=np.concatenate((XvnlY,XvnrY),axis=1)\n",
    "    print(XvrlY.shape)\n",
    "    \n",
    "    \n",
    "    np.random.shuffle(XsrlY.T)\n",
    "    np.random.shuffle(XtrlY.T)\n",
    "    np.random.shuffle(XvrlY.T)\n",
    "    \n",
    "    scores=[]\n",
    "    scoreR=[]\n",
    "    scoreL=[]\n",
    "\n",
    "    y = XsrlY[-1,:].reshape(num_compl+num_compr)\n",
    "\n",
    "    for i in range(100):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i+1)\n",
    "        knn.fit(XsrlY[0:-1,:].T,y)\n",
    "        #rscore=knn.score(Xtnr.T,np.ones(num_compr))\n",
    "        #lscore=knn.score(Xtnl.T,np.zeros(num_compl))\n",
    "        #scores.append((rscore*num_compr+lscore*num_compl)/(num_compr+num_compl))\n",
    "        #scoreR.append(rscore)\n",
    "        #scoreL.append(lscore)\n",
    "        scores.append(knn.score(XvrlY[0:-1,:].T, XvrlY[-1,:].reshape(num_compl+num_compr)))\n",
    "\n",
    "    maxim = np.argmax(scores)+1\n",
    "    knn = KNeighborsClassifier(maxim)\n",
    "    knn.fit(XsrlY[0:-1,:].T,y)\n",
    "    print(XtrlY[0:-1,:].T.shape)\n",
    "    print(XtrlY[-1,:].reshape(num_compl+num_compr))\n",
    "                              \n",
    "    testScore=knn.score(XtrlY[0:-1,:].T, XtrlY[-1,:].reshape(num_compl+num_compr))\n",
    "\n",
    "    print(testScore)\n",
    "    return testScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoresNormal(chunkRights,chunkLefts,chunkRightt,chunkLeftt):\n",
    "\n",
    "    ones=np.ones((1,len(chunkRights[0])))\n",
    "    chunkTrainR=np.concatenate((chunkRights,ones),axis=0)\n",
    "\n",
    "\n",
    "    ones=np.ones((1,len(chunkRightt[0])))\n",
    "    chunkTestR=np.concatenate((chunkRightt,ones),axis=0)\n",
    "\n",
    "\n",
    "    zeros=np.zeros((1,len(chunkLefts[0])))\n",
    "    chunkTrainL=np.concatenate((chunkLefts,zeros),axis=0)\n",
    "\n",
    "    zeros=np.zeros((1,len(chunkLeftt[0])))\n",
    "    chunkTestL=np.concatenate((chunkLeftt,zeros),axis=0)\n",
    "    chunkTrain = np.concatenate((chunkTrainL,chunkTrainR), axis=1)\n",
    "    chunkTest = np.concatenate((chunkTestL,chunkTestR), axis=1)\n",
    "    \n",
    "    np.random.shuffle(chunkTrain.T)\n",
    "    np.random.shuffle(chunkTest.T)\n",
    "    scores=[]\n",
    "    for i in np.arange(5,15):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i)\n",
    "        knn.fit(chunkTrain[0:-1,:].T,chunkTrain[-1,:])\n",
    "        scores.append(knn.score(chunkTest[0:-1,:].T,chunkTest[-1,:]))\n",
    "    print(np.max(scores))\n",
    "    return float(np.max(scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold (Xs,Xv,Xt,Ys,Yv,Yt):\n",
    "    chunkXsn,chunkXvn,chunkXtn,chunkYs,chunkYv,chunkYt=cutInChunks(Xs,Xv,Xt,Ys,Yv,Yt)\n",
    "    chunkRights,chunkLefts,chunkRightt,chunkLeftt=rightLeft(chunkXsn,chunkXtn,chunkYs,chunkYt)\n",
    "    _,_,chunkRightv,chunkLeftv = rightLeft(chunkXsn,chunkXvn,chunkYs,chunkYv)\n",
    "    \n",
    "    chunkRighttp, chunkLefttp=padding(chunkRightt,chunkLeftt,chunkRights,chunkLefts,pad=0)\n",
    "    chunkRightvp, chunkLeftvp=padding(chunkRightv,chunkLeftv,chunkRights,chunkLefts,pad=0)\n",
    "    \n",
    "    num_compr=PCselection(chunkRights)\n",
    "    num_compl=PCselection(chunkLefts)\n",
    "    \n",
    "    Xsnr, Xtnr = TCA(chunkRights,chunkRighttp,num_components=num_compr)\n",
    "    Xsnl, Xtnl = TCA(chunkLefts,chunkLefttp,num_components=num_compl)\n",
    "    \n",
    "    _,Xvnr=TCA(chunkRights,chunkRightvp,num_components=num_compr)\n",
    "    _,Xvnl=TCA(chunkRights,chunkRightvp,num_components=num_compl)\n",
    "    \n",
    "    #Xsnr,Xsnl,Xtnr,Xtnl= scaleAfter(Xsnr,Xsnl,Xtnr,Xtnl)\n",
    "    print(\"TCA completed\")\n",
    "    scoreTCA =getScores(Xsnr,Xsnl,Xtnr,Xtnl,Xvnr,Xvnl,num_compr,num_compl)\n",
    "    scoreN=scoresNormal(chunkRights,chunkLefts,chunkRightt,chunkLeftt)\n",
    "    print(\"fold\")\n",
    "    return  scoreTCA,scoreN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels = 64\n",
    "patTrain = 12\n",
    "patVal=1\n",
    "patTest=1\n",
    "\n",
    "\n",
    "vector = np.arange(0,896)\n",
    "vector = list(vector)\n",
    "vectorY = np.arange(0,14)\n",
    "scoresTCA=[]\n",
    "scoresN=[]\n",
    "for i in np.arange(0,13):  \n",
    "    isin = np.logical_not(np.isin(vector,vector[(patTrain-i)*channels:(patTrain+patVal+patTest-i)*channels]))\n",
    "    indx = np.array(np.where(isin==True))\n",
    "    indx=indx.reshape((len(indx[0])))\n",
    "\n",
    "\n",
    "    Xs = eegMatrixS[:,indx]\n",
    "    \n",
    "    #Xv = eegMatrixS[:,patTrain*channels:(patTrain+patVal)*channels]\n",
    "    Xv = np.zeros((Xs.shape))\n",
    "    Xv[:,0:patVal*channels] = eegMatrixS[:,(patTrain-i)*channels:(patTrain+patVal-i)*channels]\n",
    "\n",
    "    Xt = np.zeros((Xs.shape))\n",
    "    Xt[:,0:patTest*channels] = eegMatrixS[:,(patTrain+patVal-i)*channels:(patTrain+patVal+patTest-i)*channels]\n",
    "\n",
    "    Yv = labels[:,patTrain-i:patTrain+patVal-i]\n",
    "\n",
    "    Yt = labels[:,patTrain+patVal-i:patTrain+patVal+patTest-i]\n",
    "    \n",
    "    isinY = np.logical_not(np.isin(vectorY,vectorY[(patTrain-i):(patTrain+patVal+patTest-i)]))\n",
    "\n",
    "    indxY = np.array(np.where(isinY==True))\n",
    "    indxY = indxY.reshape(patTrain)\n",
    "\n",
    "    Ys = labels[:,indxY]\n",
    "    scoreTCA,scoreNormal=fold(Xs,Xv,Xt,Ys,Yv,Yt)\n",
    "    scoresTCA.append(scoreTCA)\n",
    "    scoresN.append(scoreNormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_COUNTER=0\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 13\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)        \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    \n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scoresTCA)\n",
    "plt.plot(np.ones(13)*np.mean(scoresTCA))\n",
    "plt.plot(scoresN)\n",
    "plt.plot(np.ones(13)*np.mean(scoresN))\n",
    "plt.title(\"With TCA vs without TCA \")\n",
    "plt.legend(['accuracy after TCA',\"accuracy without TCA\",\"mean accuracy with TCA\", \"mean accuracy without TCA\" ],loc=\"upper left\")\n",
    "\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
